## Spark Sql

> Spark sql是spark用于处理结构化数据处理的Spark模块。

### 1 前传

#### 1.1 由来

Spark sql的前身是Shark，给熟悉结构化数据库管理工具，但是不理解MapReduce的技术人员提供快速上手的工具。Hive 是早期唯一运行在 Hadoop 上的 SQL-on-Hadoop 工具。但是 MapReduce 计算过程中大量的中间磁盘落地过程消耗了大量的 I/O，降低的运行效率，为了提高 SQL-on-Hadoop的效率，大量的 SQL-on-Hadoop 工具开始产生：

- Drill
- Impala
- Shark

Shark是伯努利实验室Spark生态环境的组件之一，是基于Hive所开发的工具，修改了内存管理，物理执行计划，执行三个模块，并使之能运行在Spark引擎之上。

![image-20210926214626869](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210926214626869.png)

但是Shark对于hive的依赖过多，制约了Spark发展。所以提出了SparkSql，抛弃了原有的Shark代码。吸取了一些优点，内存列存储（In-Memory Columnar Storage）、Hive兼容性等，重新开发了SparkSQL代码；由于摆脱了对Hive的依赖性，SparkSQL无论在数据兼容、性能优化、组件扩展方面都得到了极大的方便。

- 兼容性：SparkSQL 不但兼容 Hive，还可以从 RDD、parquet 文件、JSON 文件中获取数据，未来版本甚至支持获取 RDBMS 数据以及 cassandra 等 NOSQL 数据。
- 性能方面： In-Memory Columnar Storage、byte-code generation 等优化技术外、将会引进 Cost Model 对查询进行动态评估、获取最佳物理计划等等。
- 组件扩展方面 无论是 SQL 的语法解析器、分析器还是优化器都可以重新定义，进行扩展。

014 年 6 月 1 日 Shark 项目和 SparkSQL 项目的主持人 Reynold Xin 宣布：停止对 Shark 的开发，团队将所有资源放 SparkSQL 项目上，至此，Shark 的发展画上了句话，但也因此发展出两个支线：SparkSQL 和 Hive on Spark。SparkSQL可以简化RDD的开发，提高开发的效率，提供了2个编程抽象：

- DataFrame
- DataSet

#### 1.2 Spark SQL的特点

- 易于整合：无缝的整合了 SQL 查询和 Spark 编程。
- 统一的数据访问：相同的方式连接不同的数据源。
- 兼容hive：在已有的仓库上直接运行SQL。
- 标准数据连接：通过JDBC和ODBC连接

#### 1.3 DataFrame

在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL 得以洞察更多的结构信息，从而对藏于 DataFrame 背后的数据源以及作用于 DataFrame 之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。同hive类似，DataFrame也支持嵌套数据类型（Struct,array,map）。

![image-20210926215645244](C:\Users\龍\AppData\Roaming\Typora\typora-user-images\image-20210926215645244.png)

DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计划通过 Spark catalyst optimiser 进行优化。比如下面一个例子:

![image-20210926215711204](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210926215711204.png)

简而言之：逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。

#### 1.4 DataSet

DataSet 是分布式数据集合。DataSet 是 Spark 1.6 中添加的一个新抽象，是 DataFrame的一个扩展。它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 SparkSQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter等等）。

- DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象
- 用户友好的 API 风格，既具有类型安全检查也具有 DataFrame 的查询优化特性；
- 用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet 中的字段名称；
-  DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。
- DataFrame 是 DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为 DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。获取数据时需要指定顺序

### 2 核心编程

> 主要在于DataFrame和DataSet模型的使用。

#### 2.1 环境

SparkCore的环境对象是SparkContext，Spark Sql也可以理解为对SparkCore的一种封装，不仅封装了模型，而且封装了环境对象。

- SQLContext：用于Spark自己提供的SQL查询。
- HiveContext：连接hive查询。

![image-20210926220743089](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210926220743089.png)

SparkSession 是 Spark 最新的 SQL 查询起始点，实质上是 SQLContext 和 HiveContext的组合，所以在 SQLContex 和 HiveContext 上可用的 API 在 SparkSession 上同样是可以使用的。SparkSession 内部封装了 SparkContext，所以计算实际上是由 sparkContext 完成的。当我们使用 spark-shell 的时候, spark框架会自动的创建一个名称叫做spark的SparkSession对象, 就像我们以前可以自动获取到一个 sc 来表示 SparkContext 对象一样

#### 2.2 DataFrame

>park SQL 的 DataFrame API 允许我们使用 DataFrame 而不用必须去注册临时表或者生成 SQL 表达式。DataFrame API 既有 transformation 操作也有 action 操作。

##### 2.2.1 构建DataFrame

1、从Spark数据源创建

![image-20210926221245391](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210926221245391.png)

2、从RDD进行转换

3、从Hive Table查询返回

##### 2.2.2 Sql语法

1、使用sql的方式查询

注：如果创建的是全局的临时view，查询的时候需要添加`global_temp.`前缀

```scala
scala> df.createGlobalTempView("people")
scala> spark.sql("SELECT * FROM global_temp.people").show()
+---+--------+
|age|username|
+---+--------+
| 20|zhangsan|
| 30| lisi|
| 40| wangwu|
+---+--------+
```

##### 2.2.3 DSL语法

> DataFrame 提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。

1、创建DataFrame

```scala
scala> val df = spark.read.json("data/user.json")
df: org.apache.spark.sql.DataFrame = [age: bigint， name: string]
```

2、查看Schema的信息

```scala
scala> df.printSchema
root
|-- age: Long (nullable = true)
|-- username: string (nullable = true)
```

3、查询信息

```scala
scala> df.select("username").show()
+--------+
|username|
+--------+
|zhangsan|
| lisi|
| wangwu|
+--------+
```

4、查看username列和age+1列

如果涉及到计算，那么每列哦都要使用$或者采用引号表达式

```scala
scala> df.select($"username",$"age" + 1).show
scala> df.select('username, 'age + 1).show()
```

5、查看"age"大于"30"的数据

```scala
scala> df.filter($"age">30).show
+---+---------+
|age| username|
+---+---------+
| 40| wangwu|
+---+---------+
```

6、如果创建的是全局临时view

##### 2.2.4 DataFrame转化RDD

如果开发过程中，我们需要RDD和DF或者DS之间相互操作，那么我们需要引入：import spark.implicits._



实际开发的过程中印版通过样例类讲`RDD`转换为`DataFrame`。

```scala
scala> case class User(name:String, age:Int)
defined class User
scala> sc.makeRDD(List(("zhangsan",30), ("lisi",40))).map(t=>User(t._1,
t._2)).toDF.show
+--------+---+
| name|age|
+--------+---+
|zhangsan| 30|
| lisi| 40|
+--------+---+
```

##### 2.2.5 DataFrame转化RDD

DataFrame实际上是对RDD的封装，所以可以直接获取内部的RDD。

```scala
scala> val df = sc.makeRDD(List(("zhangsan",30), ("lisi",40))).map(t=>User(t._1,
t._2)).toDF
df: org.apache.spark.sql.DataFrame = [name: string, age: int]
scala> val rdd = df.rdd
rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[46]
at rdd at <console>:25
scala> val array = rdd.collect
array: Array[org.apache.spark.sql.Row] = Array([zhangsan,30], [lisi,40])
```

注：此时RDD的存储类型为ROW

#### 2.3 DataSet

##### 2.3.1 创建

1、使用样例类创建DataSet

```scala
scala> case class Person(name: String, age: Long)
defined class Person
scala> val caseClassDS = Seq(Person("zhangsan",2)).toDS()
caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: Long]
scala> caseClassDS.show
+---------+---+
| name|age|
+---------+---+
| zhangsan| 2|
+---------+---+
```

2、使用基本类型的序列创建DataSet

```scala
scala> val ds = Seq(1,2,3,4,5).toDS
ds: org.apache.spark.sql.Dataset[Int] = [value: int]
scala> ds.show
+-----+
|value|
+-----+
| 1|
| 2|
| 3|
| 4|
| 5|
+-----+
```

##### 2.3.2 RDD转换为DataSet

SparkSQL 能够自动将包含有 case 类的 RDD 转换成 DataSet，case 类定义了 table 的结构，case 类属性通过反射变成了表的列名。Case 类可以包含诸如 Seq 或者 Array 等复杂的结构。

```scala
scala> case class User(name:String, age:Int)
defined class User
scala> sc.makeRDD(List(("zhangsan",30), ("lisi",49))).map(t=>User(t._1,
t._2)).toDS
res11: org.apache.spark.sql.Dataset[User] = [name: string, age: int
```

##### 2.3.3 DataSet转换为RDD

dataset是对RDD的封装，直接获取即可

```scala
scala> case class User(name:String, age:Int)
defined class User
scala> sc.makeRDD(List(("zhangsan",30), ("lisi",49))).map(t=>User(t._1,
t._2)).toDS
res11: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
scala> val rdd = res11.rdd
rdd: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[51] at rdd at
<console>:25
scala> rdd.collect
res12: Array[User] = Array(User(zhangsan,30), User(lisi,49)
```

##### 2.3.4 DataSet和DataFrame的转换

DataFrame 其实是 DataSet 的特例（类型为row的dataset），所以它们之间是可以互相转换的。

1、DataFrame 转换为 DataSet：缺少类型

```scala
scala> case class User(name:String, age:Int)
defined class User
scala> val df = sc.makeRDD(List(("zhangsan",30),
("lisi",49))).toDF("name","age")
df: org.apache.spark.sql.DataFrame = [name: string, age: int]
scala> val ds = df.as[User]
ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
```

2、DataSet 转换为 DataFrame

```scala
scala> val ds = df.as[User]
ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
scala> val df = ds.toDF
df: org.apache.spark.sql.DataFrame = [name: string, age: int]
```

#### 2.4 关系

##### 2.4.1 版本信息

- Spark1.0 => RDD
- Spark1.3 => DataFrame
- Spark1.6 => Dataset

##### 2.4.2 转换关系

![image-20210926224810849](C:\Users\龍\AppData\Roaming\Typora\typora-user-images\image-20210926224810849.png)

##### 2.4.3 三者共性

- RDD、DataFrame、DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数据提供便利。
- 者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到Action 如 foreach 时，三者才会开始遍历运算。
- 三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出

##### 2.4.4 三者区别

1、RDD

- RDD 一般和 spark mllib 同时使用
- RDD 不支持 sparksql 操作

2、DataFrame

- 与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值。
- DataFrame 与 DataSet 一般不与 spark mllib 同时使用
- DataFrame 与 DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能注册临时表/视窗，进行 sql 语句操作.
- DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv。

3、DataSet

- Dataset 和 DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同。DataFrame 其实就是 DataSet 的一个特例 type DataFrame = Dataset[Row]
- 而 Dataset 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息

### 3 Idea 开发

#### 3.1 base

```scala
/**
 * @author : code1997
 * @date : 2021/9/26 22:52
 */
object SparkSql_Base {

  def main(args: Array[String]): Unit = {
    import org.apache.spark.SparkConf
    import org.apache.spark.rdd.RDD
    import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}

    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("spark-session")
    val session: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    //注意引入隐士转换的规则，才可以实现rdd==>dataframe
    import session.implicits._
    //读取json文件的方式创建DataFrame
    val dataFrame: DataFrame = session.read.json("data/spark-sql/base/User.json")
    dataFrame.show()
    //sql风格
    dataFrame.createTempView("user")
    session.sql("select username from user").show()
    //DSL的方式
    dataFrame.select("username").show()
    //RDD==>DataFrame=>Dataset
    val rdd: RDD[(Int, String, Int)] = session.sparkContext.makeRDD(List((1, "zhangsan", 30), (2, "lisi", 28), (3, "wangwu", 20)))
    val dataFrame1: DataFrame = rdd.toDF("id", "username", "age")
    val dataset1: Dataset[User] = dataFrame1.as[User]

    //dataset==>dataframe==>rdd
    val dataFrame2: DataFrame = dataset1.toDF()
    val rdd2: RDD[Row] = dataFrame2.rdd
    rdd2.foreach(row => println(row.getString(1)))

    //RDD==>Dataset:利用隐式转换
    rdd.map {
      case (id, name, age) => User(id, name, age)
    }.toDS()

    session.close()
  }
}

case class User(id: Int, username: String, age: Int) {

}
```

#### 3.2 UDF

用户可以通过 `spark.udf` 功能添加自定义函数，实现自定义功能。

##### 3.2.1 入门案例

如果我想要给username加上前缀，我们应该如何做？

```scala
def main(args: Array[String]): Unit = {
  import org.apache.spark.SparkConf
  import org.apache.spark.sql.{DataFrame, SparkSession}

  val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("spark-session-udf")
  val session: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
  //注意引入隐士转换的规则，才可以实现rdd==>dataframe
  import session.implicits._
  //读取json文件的方式创建DataFrame
  val dataFrame: DataFrame = session.read.json("data/spark-sql/base/User.json")
  dataFrame.createTempView("User")
  ////如果我想要给username加上前缀，我们应该如何做？自定义udf
  session.udf.register("prefixUsername", (name: String) => "Name:" + name)
  session.sql("select prefixUsername(username),age from user").show()
  session.close()
}
```

##### 3.2.2 UDAF

强类型的 Dataset 和弱类型的 DataFrame 都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承 `UserDefinedAggregateFunction` 来实现用户自定义弱类型聚合函数。从 Spark3.0 版本后，`UserDefinedAggregateFunction` 已经不推荐使用了。可以统一采用强类型聚合函数`Aggregator`。

1、弱类型方式

```scala
/**
 * 计算年龄的平均值
 *
 * @author : code1997
 * @date : 2021/9/27 0:00
 */
object SparkSql_udaf_old {

  import org.apache.spark.sql.expressions.UserDefinedAggregateFunction
  import org.apache.spark.sql.{DataFrame, SparkSession}
  import org.apache.spark.{Aggregator, SparkConf}

  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("spark-session-udf")
    val session: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    //注意引入隐士转换的规则，才可以实现rdd==>dataframe
    //读取json文件的方式创建DataFrame
    val dataFrame: DataFrame = session.read.json("data/spark-sql/base/User.json")
    dataFrame.createTempView("User")
    session.udf.register("ageAvg", new MyAvgUDAF)
    session.sql("select ageAvg(age) from user").show()
    session.close()
  }

  //输入数据类型
  case class User01(username: String, age: Long)

  //缓存类型
  case class AgeBuffer(var sum: Long, var count: Long)

  /**
   * 自定义聚合函数类：计算平均值
   * 弱类型的方式：需要根据位置来获取值，不利于使用
   */
  class MyAvgUDAF extends UserDefinedAggregateFunction {

    import org.apache.spark.sql.Row
    import org.apache.spark.sql.expressions.MutableAggregationBuffer
    import org.apache.spark.sql.types.{DataType, LongType, StructType}

    //输入结构
    override def inputSchema: StructType = {
      import org.apache.spark.sql.types.{LongType, StructField}
      StructType(
        Array(
          StructField("age", LongType)
        )
      )
    }

    //缓冲区结构
    override def bufferSchema: StructType = {
      import org.apache.spark.sql.types.{LongType, StructField}
      StructType(
        Array(
          StructField("total", LongType),
          StructField("count", LongType)
        )
      )
    }

    //函数计算结果的类型
    override def dataType: DataType = LongType

    //函数的稳定性:传入相同的参数，结果是否是一样的
    override def deterministic: Boolean = true

    //缓冲区初始化
    override def initialize(buffer: MutableAggregationBuffer): Unit = {
      buffer(0) = 0L
      buffer(1) = 0L
      //buffer.update(0,0L)
      //buffer.update(1,0L)
    }

    //根据输入的值来更新缓冲区数据
    override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
      buffer.update(0, buffer.getLong(0) + input.getLong(0))
      buffer.update(1, buffer.getLong(1) + 1)
    }

    //缓冲区如何合并，默认情况下，1是结果，2是新来的
    override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
      buffer1.update(0, buffer1.getLong(0) + buffer2.getLong(0))
      buffer1.update(1, buffer1.getLong(1) + buffer2.getLong(1))
    }

    //计算平均值
    override def evaluate(buffer: Row): Any = {
      buffer.getLong(0) / buffer.getLong(1)
    }
  }
}
```

2、强类型的方式

> Spark3.0 版本可以采用强类型的 Aggregator 方式代替 UserDefinedAggregateFunction

```scala
/**
 * 计算年龄的平均值
 *
 * @author : code1997
 * @date : 2021/9/27 0:00
 */
object SparkSql_udaf {

  import org.apache.spark.SparkConf
  import org.apache.spark.sql.expressions.Aggregator
  import org.apache.spark.sql.{DataFrame, SparkSession}

  def main(args: Array[String]): Unit = {
    import org.apache.spark.sql.functions
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("spark-session-udf")
    val session: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    //注意引入隐士转换的规则，才可以实现rdd==>dataframe
    //读取json文件的方式创建DataFrame
    val dataFrame: DataFrame = session.read.json("data/spark-sql/base/User.json")
    dataFrame.createTempView("User")
    session.udf.register("MyAgeAvg2", functions.udaf(new MyAgeAvg2))
    session.sql("select MyAgeAvg2(age) from user").show()
    session.close()
  }

  //输入数据类型
  case class User01(username: String, age: Long)

  //缓存类型
  case class AgeBuffer2(var sum: Long, var count: Long)

  /**
   * 自定义聚合函数类：计算平均值
   * 1)继承Aggregator
   * IN:输入类型
   * BUF:缓冲区的数据类型Buff
   * OUT:输出的数据类型Long
   * 2）重写方法
   */
  class MyAgeAvg2 extends Aggregator[Long, AgeBuffer2, Long] {

    import org.apache.spark.sql.Encoder
    import org.apache.spark.sql.Encoders

    //初始化值
    override def zero: AgeBuffer2 = {
      AgeBuffer2(0L, 0L)
    }

    //跟胡输入数据更新缓冲区的数据
    override def reduce(b: AgeBuffer2, a: Long): AgeBuffer2 = {
      b.sum = b.sum + a
      b.count = b.count + 1
      b
    }

    //合并缓冲区
    override def merge(b1: AgeBuffer2, b2: AgeBuffer2): AgeBuffer2 = {
      b1.sum = b1.sum + b2.sum
      b1.count = b1.count + b2.count
      b1
    }

    override def finish(reduction: AgeBuffer2): Long = {
      reduction.sum / reduction.count
    }

    //固定写法：自定义类使用product
    override def bufferEncoder: Encoder[AgeBuffer2] = {
      Encoders.product
    }

    //固定写法：scala的使用scala
    override def outputEncoder: Encoder[Long] = {
      Encoders.scalaLong
    }
  }
}
```

#### 3.3 数据的加载和保存

SparkSQL 提供了通用的保存数据和数据加载的方式。这里的通用指的是使用相同的API，根据不同的参数读取和保存不同格式的数据，SparkSQL 默认读取和保存的文件格式为 parquet

##### 3.3.1 加载数据

`spark.read.load`施加在数据的通用方法

```scala
scala> spark.read.format("…")[.option("…")].load("…")
```

- format("…")：指定加载的数据类型，包括"csv"、"jdbc"、"json"、"orc"、"parquet"和
  "textFile"
- load("…")：在"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"格式下需要传入加载
  数据的路径。
- option("…")：在"jdbc"格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable我们前面都是使用 read API 先把文件加载到 DataFrame 然后再查询，其实，我们也可以直接在文件上进行查询: 文件格式.`

```scala
scala>spark.sql("select * from json.`/opt/module/data/user.json`").show
```

##### 3.3.2 保存数据

`df.write.save` 是保存数据的通用方法

```scala
scala>df.write.format("…")[.option("…")].save("…")
```

保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置，但是这个方法是没有加锁的，不是原子性操作。

- SaveMode.ErrorIfExists(default) "error"(default) 如果文件已经存在则抛出异常
- SaveMode.Append "append" 如果文件已经存在则追加
- SaveMode.Overwrite "overwrite" 如果文件已经存在则覆盖
- SaveMode.Ignore "ignore" 如果文件已经存在则忽略

##### 3.3.3 文件类型

1、Parquet

Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式存储格式。数据源为 Parquet 文件时，Spark SQL 可以方便的执行所有的操作，不需要使用 format。修改配置项 spark.sql.sources.default，可修改默认数据源格式。

1）加载数据

```scala
scala> val df = spark.read.load("examples/src/main/resources/users.parquet")
scala> df.show
```

2）保存数据

```scala
scala> var df = spark.read.json("/opt/module/data/input/people.json")
//保存为 parquet 格式
scala> df.write.mode("append").save("/opt/module/data/output")
```

2、json

Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个 Dataset[Row]. 可以通过 SparkSession.read.json()去加载 JSON 文件。但是读取的json文件不是传统的json文件，每一行都应该是一个json串。

3、CSV

Spark SQL 可以配置 CSV 文件的列表信息，读取 CSV 文件,CSV 文件的第一行设置为数据列

```scala
spark.read.format("csv").option("sep",";").option("inferSchema","true").option("header","true").load("data/user.csv")
```

4、MySQL

Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建 DataFrame，通过对DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。如果使用 spark-shell 操作，可在启动 shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类

```scala
bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar
```

1）导入依赖

```xml
<dependency>
	<groupId>mysql</groupId>
	<artifactId>mysql-connector-java</artifactId>
	<version>5.1.27</version>
</dependency>
```

2）读取数据

```scala
val conf: SparkConf = new
SparkConf().setMaster("local[*]").setAppName("SparkSQL")
//创建 SparkSession 对象
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
import spark.implicits._
//方式 1：通用的 load 方法读取
spark.read.format("jdbc")
.option("url", "jdbc:mysql://hadoop02:3306/spark-sql")
.option("driver", "com.mysql.jdbc.Driver")
.option("user", "root")
.option("password", "123123")
.option("dbtable", "user")
.load().show
//方式 2:通用的 load 方法读取 参数另一种形式
spark.read.format("jdbc")
.options(Map("url"->"jdbc:mysql://linux1:3306/spark-sql?user=root&password=123123","dbtable"->"user","driver"->"com.mysql.jdbc.Driver")).load().show
//方式 3:使用 jdbc 方法读取
val props: Properties = new Properties()
props.setProperty("user", "root")
props.setProperty("password", "123123")
val df: DataFrame = spark.read.jdbc("jdbc:mysql://linux1:3306/spark-sql","user", props)
df.show
//释放资源
spark.stop()
```

3）写入数据

```scala
case class User2(name: String, age: Long)
。。。
val conf: SparkConf = new
SparkConf().setMaster("local[*]").setAppName("SparkSQL")
//创建 SparkSession 对象
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
import spark.implicits._
val rdd: RDD[User2] = spark.sparkContext.makeRDD(List(User2("lisi", 20),User2("zs", 30)))
val ds: Dataset[User2] = rdd.toDS
//方式 1：通用的方式 format 指定写出类型
ds.write
.format("jdbc")
.option("url", "jdbc:mysql://linux1:3306/spark-sql")
.option("user", "root")
.option("password", "123123")
.option("dbtable", "user")
.mode(SaveMode.Append)
.save()
//方式 2：通过 jdbc 方法
val props: Properties = new Properties()
props.setProperty("user", "root")
props.setProperty("password", "123123")
ds.write.mode(SaveMode.Append).jdbc("jdbc:mysql://linux1:3306/spark-sql",
"user", props)
//释放资源
spark.stop()
```

5、hive

Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL 编译时可以包含 Hive 支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的一点是，如果要在 Spark SQL 中包含Hive 的库，并不需要事先安装 Hive。一般来说，最好还是在编译 Spark SQL 时引入 Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。

若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到Spark 的配置文件目录中($SPARK_HOME/conf)。即使没有部署好 Hive，Spark SQL 也可以运行。 需要注意的是，如果你没有部署好 Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。spark-shell 默认是 Hive 支持的；代码中是默认不支持的，需要手动指定（加一个参数即可）

1）内嵌hive

如果使用 Spark 内嵌的 Hive, 则什么都不用做, 直接使用即可，Hive 的元数据存储在 derby 中, 默认仓库地址:$SPARK_HOME/spark-warehouse

2）外部hive

如果想连接外部已经部署好的 Hive，需要通过以下几个步骤：

- Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下
- 把 Mysql 的驱动 copy 到 jars/目录下
- 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下
- 重启 spark-shell

3）Spark SQL CLI

park SQL CLI 可以很方便的在本地运行 Hive 元数据服务以及从命令行执行查询任务。在Spark 目录下执行如下命令启动 Spark SQL CLI，直接执行 SQL 语句，类似一 Hive 窗口

```scala
bin/spark-sql
```

4）Spark beeline

Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。Spark Thrift Server 的目的也只是取代 HiveServer2，因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据。

如果想连接 Thrift Server，需要通过以下几个步骤：

- Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下
- 把 Mysql 的驱动 copy 到 jars/目录下
- 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下
- 启动 Thrift Server

```scala
sbin/start-thriftserver.sh
#使用 beeline 连接 Thrift Server
bin/beeline -u jdbc:hive2://linux1:10000 -n root
```

5）代码操作Hive

依赖

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.12</artifactId>
    <version>3.0.0</version>
</dependency>
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-hive_2.12</artifactId>
    <version>3.0.0</version>
</dependency>
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-exec</artifactId>
    <version>1.2.1</version>
</dependency>
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>5.1.27</version>
</dependency>
```

导入`hive-site.xml`

编写代码：

```scala
val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("spark-session-udf")
val spark: SparkSession = SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()
spark.sql("show tables").show()
spark.close()
```

如果出现没有权限，那么我们可以添加hadoop用户，例如：`root`

```scala
System.setProperty("HADOOP_USER_NAME", "root")
```

