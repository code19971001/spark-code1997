## Spark核心编程

### 1 RDD

#### 1.1 什么是RDD？

RDD是弹性分布式数据集，是spark最基本的数据处理模型，代表一个弹性的，不可变的，可分区的，元素可并行计算的集合。

- 弹性：
  - 存储的弹性：内存与磁盘的自动切换
  - 容错的弹性：数据丢失可以自动恢复
  - 计算的弹性：计算出错重试机制
  - 分片的弹性：可根据需要重新分片，实际上我们理解为分区。
- 分布式：数据存储在大数据集群不同节点上，数据来源应该是分布式的环境中。
- 数据集：RDD 封装了计算逻辑，并不保存数据
- 数据抽象：RDD 是一个抽象类，需要子类具体实现
- 不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装新的计算逻辑
- 可分区、并行计算

图解：

![image-20210922215251947](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210922215251947.png)

#### 1.2 核心属性

```scala
Internally, each RDD is characterized by five main properties:
A list of partitions
A function for computing each split
A list of dependencies on other RDDs
Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)
```

1）分区列表

RDD 数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。

```scala

  /**
   * Implemented by subclasses to return the set of partitions in this RDD. This method will only
   * be called once, so it is safe to implement a time-consuming computation in it.
   *
   * The partitions in this array must satisfy the following property:
   *   `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }`
   */
  protected def getPartitions: Array[Partition]
```

2）分区计算函数

Spark 在计算时，是使用分区函数对每一个分区进行计算

```scala
  // =======================================================================
  // Methods that should be implemented by subclasses of RDD
  // =======================================================================

  /**
   * :: DeveloperApi ::
   * Implemented by subclasses to compute a given partition.
   */
  @DeveloperApi
  def compute(split: Partition, context: TaskContext): Iterator[T]
```

3）RDD的依赖关系

RDD 是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个 RDD 建立依赖关系

```scala

  /**
   * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only
   * be called once, so it is safe to implement a time-consuming computation in it.
   */
  protected def getDependencies: Seq[Dependency[_]] = deps

```

4）分区器（可选）

当数据为 KV 类型数据时，可以通过设定分区器自定义数据的分区

```scala

  /** Optionally overridden by subclasses to specify how they are partitioned. */
  @transient val partitioner: Option[Partitioner] = None
```

5）首选位置（可选）

计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算，需要计算将task发送给哪一个节点效率最优-->移动数据不如移动计算。

```scala

  /**
   * Optionally overridden by subclasses to specify placement preferences.
   */
  protected def getPreferredLocations(split: Partition): Seq[String] = Nil
```

#### 1.3 执行原理

计算的角度来看，数据处理需要计算资源，包含内存和cpu以及计算模型，真正执行的时候需要将计算资源和计算模型进行协调。Spark框架在执行的时候，先申请资源，然后将应用程序的数据处理逻辑分解成计算任务，然后发送给分配资源的计算节点上，然后按照计算模型进行数据计算，最终得到计算结果。

1）启动yarn集群环境

![image-20210922221137046](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210922221137046.png)

2）Spark申请资源创建调度节点以及计算节点

![image-20210922221131467](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210922221131467.png)

3）Spark框架根据需求将计算逻辑根据分区划分为不同的任务

![image-20210922221123075](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210922221123075.png)

4）调用节点根据节点状态发送task到指定的计算节点计算

![image-20210922221112814](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210922221112814.png)

RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给
Executor 节点执行计算

### 2 基础编程

#### 2.1 创建RDD

##### 2.1.1 基于集合

```scala
/**
 * 基于内存构建RDD
 * 1)parallelize
 * 2)makeRDD:实际上调用的就是parallelize方法.
 *
 * @author : code1997
 * @date : 2021/9/22 22:13
 */
object RDD_Memory {

  def main(args: Array[String]): Unit = {
    //local[*]根据计算机的硬件条件.
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val sc = new SparkContext(sparkConf)
    val seq = Seq[Int](1, 2, 3, 4)
    val rdd1: RDD[Int] = sc.parallelize(seq)
    val rdd2: RDD[Int] = sc.makeRDD(seq)
    rdd1.collect().foreach(println)
    rdd2.collect().foreach(println)
    sc.stop()
  }
}
```

##### 2.1.2 基于文件

```scala
/**
 * 基于文件创建RDD
 * textFile:一次读取文件中的一行数据.
 * wholeTextFiles：一次读取一个文件，返回值是(文件路径,整个文件内容)
 *
 * @author : code1997
 * @date : 2021/9/22 22:25
 */
object RDD_File {

  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val sc = new SparkContext(sparkConf)
    //textFile不仅仅可以指向具体的文件，而且可仅指向path，也可以使用通配符以及分布式存储系统路径，比如hdfs
    val rdd1: RDD[String] = sc.textFile("data/spark-core/wordcount/1.txt")
    val rdd2: RDD[String] = sc.textFile("E:\\projects\\ideacode\\atguigu\\spark-code1997\\data\\spark-core\\wordcount\\1.txt")
    rdd1.collect().foreach(println)
    rdd2.collect().foreach(println)
    val rdd3: RDD[(String,String)] = sc.wholeTextFiles("data/spark-core/wordcount")
    rdd3.collect().foreach(println)
    sc.stop()
  }

}
```

##### 2.1.3 分区和并行度

1）并行度

默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建 RDD 时指定。如果不传递，就会使用默认值，也称为默认并行度。

```scala
    //local[*]根据计算机的硬件条件.
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val sc = new SparkContext(sparkConf)
    val seq = Seq[Int](1, 2, 3, 4)

    val rdd: RDD[Int] = sc.makeRDD(seq,2);
    rdd.saveAsTextFile("data/spark-core/output/RDD_Memory")
    sc.stop()
```

local模式默认并行度：从`sparkConfig`中获取参数`spark.default.parallelism`，如果获取不到就使用本地最大的运行核数

```scala
  override def defaultParallelism(): Int =
    scheduler.conf.getInt("spark.default.parallelism", totalCores)
```

2）分区

1、读取内存集合数据，数据分区的规则：如果分区很多，但是数据不多，如何处理？？

```scala
// TODO: Right now, each split sends along its full data, even if later down the RDD chain it gets
// cached. It might be worthwhile to write the data to a file in the DFS and read it in the split
// instead.
// UPDATE: A parallel collection can be checkpointed to HDFS, which achieves this goal.

override def getPartitions: Array[Partition] = {
  val slices = ParallelCollectionRDD.slice(data, numSlices).toArray
  slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray
}
```

核心逻辑：`positions`方法，对每个分区求出[start,end).

```scala
private object ParallelCollectionRDD {
  /**
   * Slice a collection into numSlices sub-collections. One extra thing we do here is to treat Range
   * collections specially, encoding the slices as other Ranges to minimize memory cost. This makes
   * it efficient to run Spark over RDDs representing large sets of numbers. And if the collection
   * is an inclusive Range, we use inclusive range for the last slice.
   */
  def slice[T: ClassTag](seq: Seq[T], numSlices: Int): Seq[Seq[T]] = {
    if (numSlices < 1) {
      throw new IllegalArgumentException("Positive number of partitions required")
    }
    // Sequences need to be sliced at the same set of index positions for operations
    // like RDD.zip() to behave as expected
    def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] = {
      (0 until numSlices).iterator.map { i =>
        val start = ((i * length) / numSlices).toInt
        val end = (((i + 1) * length) / numSlices).toInt
        (start, end)
      }
    }
    seq match {
      case r: Range =>
        positions(r.length, numSlices).zipWithIndex.map { case ((start, end), index) =>
          // If the range is inclusive, use inclusive range for the last slice
          if (r.isInclusive && index == numSlices - 1) {
            new Range.Inclusive(r.start + start * r.step, r.end, r.step)
          }
          else {
            new Range(r.start + start * r.step, r.start + end * r.step, r.step)
          }
        }.toSeq.asInstanceOf[Seq[Seq[T]]]
      case nr: NumericRange[_] =>
        // For ranges of Long, Double, BigInteger, etc
        val slices = new ArrayBuffer[Seq[T]](numSlices)
        var r = nr
        for ((start, end) <- positions(nr.length, numSlices)) {
          val sliceSize = end - start
          slices += r.take(sliceSize).asInstanceOf[Seq[T]]
          r = r.drop(sliceSize)
        }
        slices
      case _ =>
        val array = seq.toArray // To prevent O(n^2) operations for List etc
        positions(array.length, numSlices).map { case (start, end) =>
            array.slice(start, end).toSeq
        }.toSeq
    }
  }
}
```

2、读取文件分区

Spark读取文件是使用hadoop的文件读取方式，我们读取的时候也可以设置最小分区数，但是真实的分区数是和数据相关的。

假设我们读取一个文件，文件内容如下，文件的大小为7个字节(每行结束结束存在回车换行)，我们设置最小分区数为2。

- 每个分区的字节大小：7/2=3。
- 计算分区数量：7/3=2余1，1/3=0.33，根据hadoop110%的分区原则。则需要加一个分区，所以分区的数量为2+1=3。
- 文件中的内容：读取文件的时候是按照offset来进行文件位置定位，前后都是闭，如果已经读取过，就后面分区不会重复读取。
  - 文件1：[0,3]：文件内容为1，2
  - 文件2：[3,6]：文件内容为3
  - 文件3：[7,7]：文件内容为空

```txt
1
2
3
```

代码：

```scala
  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("rdd-file")
    val sc = new SparkContext(sparkConf)
    val rdd: RDD[String] = sc.textFile("data/spark-core/wordcount/1.txt", 2)
    rdd.saveAsTextFile("data/spark-core/output/RDD_File_Partition")
    sc.stop()
  }
```

分区截图：

![image-20210923210944425](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210923210944425.png)

### 2.2 RDD算子

RDD算子在只有转换操作的情况下不会执行，只有遇到行动算子才会执行。

从认知心理学上认为解决问题实际上就是问题的状态进行改变。

- 问题开始->操作(算子)->问题(中间态)->操作(算子)->问题解决

#### 2.2.1 转换算子

rdd根据数据处理方式的不同将算子整体上分为value类型，双value类型和key-value类型

##### 2.2.1.1 value

1、map

```scala

  /**
   * Return a new RDD by applying a function to all elements of this RDD.
   */
  def map[U: ClassTag](f: T => U): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.map(cleanF))
  }
```

test：

```scala
    val rdd: RDD[Int] = sc.makeRDD(List[Int](1, 2, 3, 4))

    def mapFunction(num: Int): Int = {
      num * 2
    }

    val mapRDD: RDD[Int] = rdd.map(mapFunction)
    mapRDD.collect().foreach(println)
    //匿名函数
    val mapRDD2 = rdd.map((num: Int) => {
      num * 2
    })
    mapRDD2.collect().foreach(println)
    //匿名函数+自简原则:方法体只有一行省去{}；参数类型可推断，去除类型；参数只有一个，使用_代替
    val mapRDD3 = rdd.map(_ * 2)
    mapRDD3.collect().foreach(println)
    sc.stop()
```

为什么说是并行计算？

- 如果分区数为1：分区内的数据有序执行，只有每一个数据走完全部的转换操作，才会执行下一个数据，这种方式的效率是比较低的。
- 如果分区数为多个：多个分区会并行执行，多个分区之间是没有顺序的。

test：有序执行

```scala
    val rdd: RDD[Int] = sc.makeRDD(List[Int](1, 2, 3, 4),1)
    val map1: RDD[Int] = rdd.map(num => {
      println(">>>>>"+num)
      num
    })
    val map2: RDD[Int] = map1.map(num => {
      println("<<<<<"+num)
      num
    })
    map2.collect()
```

test：多个分区，分区之间无序执行

```scala
val rdd: RDD[Int] = sc.makeRDD(List[Int](1, 2, 3, 4),3)
val map1: RDD[Int] = rdd.map(num => {
  println(">>>>>"+num)
  num
})
val map2: RDD[Int] = map1.map(num => {
  println("<<<<<"+num)
  num
})
map2.collect()

console:
>>>>>2
<<<<<2
>>>>>3
<<<<<3
>>>>>4
<<<<<4
>>>>>1
<<<<<1
```

2、mapPartitions

可以以分区为单位进行数据转换操作，但是会将整个分区的数据加载到内存中进行引用，处理完的数据不会被释放掉，在内存比较小，数据量比较大的场景之下，很容易出现内存溢出。

函数签名：

```scala

  /**
   * Return a new RDD by applying a function to each partition of this RDD.
   *
   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which
   * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.
   */
  def mapPartitions[U: ClassTag](
      f: Iterator[T] => Iterator[U],
      preservesPartitioning: Boolean = false): RDD[U] = withScope {
    val cleanedF = sc.clean(f)
    new MapPartitionsRDD(
      this,
      (_: TaskContext, _: Int, iter: Iterator[T]) => cleanedF(iter),
      preservesPartitioning)
  }
```

test：>>>>>只会打印两次

```scala
    val rdd: RDD[Int] = sc.makeRDD(List[Int](1, 2, 3, 4),2)
    val map1: RDD[Int] = rdd.mapPartitions(iter => {
      println(">>>>>")
      iter.map(_*2)
    })
    map1.collect()
```

test：求每个分区的最大值

```scala
    //返回每个分区中的最大值:应该是2和4
    val map2: RDD[Int] = rdd.mapPartitions(iter => {
      List(iter.max).iterator
    })
    map2.collect().foreach(println)
```

和map的区别：

- 数据处理角度：map是分区内一个数据一个数据执行，类似于串行；mapPartition一次处理一个分区，类似于批操作。
- 性能角度：map分区内是串行操作，性能比较低；mapPartition性能比较高，但是会长时间占用内存，所以内存有限的情况下，不推荐使用。
- 功能：map逐个对元素进行操作，返回数不多不少；mapPartition返回迭代器对象，没有要求数量。

3、mapPartitionsWithIndex

函数签名：会传入分区的index

```scala
/**
 * Return a new RDD by applying a function to each partition of this RDD, while tracking the index
 * of the original partition.
 *
 * `preservesPartitioning` indicates whether the input function preserves the partitioner, which
 * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.
 */
def mapPartitionsWithIndex[U: ClassTag](
    f: (Int, Iterator[T]) => Iterator[U],
    preservesPartitioning: Boolean = false): RDD[U] = withScope {
  val cleanedF = sc.clean(f)
  new MapPartitionsRDD(
    this,
    (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter),
    preservesPartitioning)
}
```

test：只要1号分区的数据

```scala
val map3: RDD[Int] = rdd.mapPartitionsWithIndex((index,iter) => {
  if(index==1){
    iter
  }else{
    //空的迭代器对象
    Nil.iterator
  }
})
```

test：返回每个数据的分区索引

```scala
val map4: RDD[(Int,Int)] = rdd.mapPartitionsWithIndex((index,iter) => {
  iter.map((index,_))
})
map4.collect().foreach(println)
```

4、flatMap

主要用于扁平映射

函数签名：

```scala
  /**
   *  Return a new RDD by first applying a function to all elements of this
   *  RDD, and then flattening the results.
   */
  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.flatMap(cleanF))
  }
```

test：合并两个list

```scala
val rdd: RDD[List[Int]] = sc.makeRDD(List[List[Int]](List(1,2),List(3,4)))
val flatRDD:RDD[Int] = rdd.flatMap(list => list)
flatRDD.collect().foreach(println)
```

test：拆分字符串

```scala
val rdd2: RDD[String] = sc.makeRDD(List("hello world","hello scala"))
val flatRDD2:RDD[String] = rdd2.flatMap(str => str.split(" "))
flatRDD2.collect().foreach(println)
```

test：合并

```scala
val rdd3 = sc.makeRDD(List(List(1, 2), 3, List(3, 4)))
val flatRDD3 = rdd3.flatMap {
  case list: List[_] => list
  case dat => List(dat)
}
flatRDD3.collect().foreach(println)
```

5、glom

将同一个分区的数据之间转换为相同类型的内存数组进行处理，分区不变，分区确定，当数据经过转换之后，分区是不会发生变化的。

函数签名：

```scala
/**
 * Return an RDD created by coalescing all elements within each partition into an array.
 */
def glom(): RDD[Array[T]] = withScope {
  new MapPartitionsRDD[Array[T], T](this, (_, _, iter) => Iterator(iter.toArray))
}
```

test：int==>array[Int]

```scala
//int==>array
val rdd: RDD[Int] = sc.makeRDD(List[Int](1, 2, 3, 4),2)
val glom:RDD[Array[Int]] = rdd.glom()
//List[Array[int]]
glom.collect().foreach(data=>println(data.mkString(",")))
```

test：分区内取最大值，然后求和

```scala
val rdd: RDD[Int] = sc.makeRDD(List[Int](1, 2, 3, 4),2)
val glomRDD2:RDD[Array[Int]] = rdd.glom()
var maxValues: RDD[Int] = glomRDD2.map(arr => {
  arr.max
})
println(maxValues.collect().sum)
```

6、groupBy

函数签名：按照指定的规则对数据进行分组，分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为shuffle，极限情况下，数据可能会被分到同一个分区中。一个组的数据在一个分区中，并不意味着一个分区中只可以有一个组。

```scala
  /**
   * Return an RDD of grouped items. Each group consists of a key and a sequence of elements
   * mapping to that key. The ordering of elements within each group is not guaranteed, and
   * may even differ each time the resulting RDD is evaluated.
   *
   * @note This operation may be very expensive. If you are grouping in order to perform an
   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`
   * or `PairRDDFunctions.reduceByKey` will provide much better performance.
   */
  def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope {
    groupBy[K](f, defaultPartitioner(this))
  }
```

test：区分奇数，偶数

```scala
val rdd: RDD[Int] = sc.makeRDD(List[Int](1, 2, 3, 4))
//区分奇数和偶数
val groupBy: RDD[(Int, Iterable[Int])] = rdd.groupBy(num => num % 2)
groupBy.collect().foreach(println)
```

test：冲apache.log中获取每个时间段的访问量

```scala
    val lines: RDD[String] = sc.textFile("data/spark-core/operator/apache.log")
    val timeRDD: RDD[(String, Iterable[(String, Int)])] = lines.map(line => {
      import java.text.SimpleDateFormat
      import java.util.Date
      val str: Array[String] = line.split(" ")
      val time: Date = new SimpleDateFormat("dd/MM/yyyy:HH:mm:ss").parse(str(3))
      (time.getHours.toString, 1)
    }).groupBy(_._1)
    //模式匹配
    timeRDD.map({
      case (hour, iter) =>
        (hour, iter.size)
    }
    ).collect().foreach(println)
```

7、filter

函数签名：根据过滤逻辑，筛选掉我们不需要的数据。但数据进行筛选过滤之后，分区不会发生改变，但是分区中的数据可能会不均衡，生产环境下，可能会出现数据倾斜(有的分区数据多，有的分区数据少)。

```scala
/**
 * Return a new RDD containing only the elements that satisfy a predicate.
 */
def filter(f: T => Boolean): RDD[T] = withScope {
  val cleanF = sc.clean(f)
  new MapPartitionsRDD[T, T](
    this,
    (_, _, iter) => iter.filter(cleanF),
    preservesPartitioning = true)
}
```

test：只要奇数

```scala
    val data: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))
    data.filter(data => data % 2 != 0).collect().foreach(println)
```

test：从apache.log文件中获取2015年5月17日的请求路径。

```scala
val lines: RDD[String] = sc.textFile("data/spark-core/operator/apache.log")
lines.filter(line => {
  val strs: Array[String] = line.split(" ")
  strs(3).startsWith("17/05/2015")
}).map(line => {
  val strs: Array[String] = line.split(" ")
  strs(6)
}).collect().foreach(println)
```

8、sample

#### 2.2.2 行动算子

