## Spark核心编程

### 1 RDD

#### 1.1 什么是RDD？

RDD是弹性分布式数据集，是spark最基本的数据处理模型，代表一个弹性的，不可变的，可分区的，元素可并行计算的集合。

- 弹性：
  - 存储的弹性：内存与磁盘的自动切换
  - 容错的弹性：数据丢失可以自动恢复
  - 计算的弹性：计算出错重试机制
  - 分片的弹性：可根据需要重新分片，实际上我们理解为分区。
- 分布式：数据存储在大数据集群不同节点上，数据来源应该是分布式的环境中。
- 数据集：RDD 封装了计算逻辑，并不保存数据
- 数据抽象：RDD 是一个抽象类，需要子类具体实现
- 不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装新的计算逻辑
- 可分区、并行计算

图解：

![image-20210922215251947](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210922215251947.png)

#### 1.2 核心属性

```scala
Internally, each RDD is characterized by five main properties:
A list of partitions
A function for computing each split
A list of dependencies on other RDDs
Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)
```

1）分区列表

RDD 数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。

```scala

  /**
   * Implemented by subclasses to return the set of partitions in this RDD. This method will only
   * be called once, so it is safe to implement a time-consuming computation in it.
   *
   * The partitions in this array must satisfy the following property:
   *   `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }`
   */
  protected def getPartitions: Array[Partition]
```

2）分区计算函数

Spark 在计算时，是使用分区函数对每一个分区进行计算

```scala
  // =======================================================================
  // Methods that should be implemented by subclasses of RDD
  // =======================================================================

  /**
   * :: DeveloperApi ::
   * Implemented by subclasses to compute a given partition.
   */
  @DeveloperApi
  def compute(split: Partition, context: TaskContext): Iterator[T]
```

3）RDD的依赖关系

RDD 是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个 RDD 建立依赖关系

```scala

  /**
   * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only
   * be called once, so it is safe to implement a time-consuming computation in it.
   */
  protected def getDependencies: Seq[Dependency[_]] = deps

```

4）分区器（可选）

当数据为 KV 类型数据时，可以通过设定分区器自定义数据的分区

```scala

  /** Optionally overridden by subclasses to specify how they are partitioned. */
  @transient val partitioner: Option[Partitioner] = None
```

5）首选位置（可选）

计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算，需要计算将task发送给哪一个节点效率最优-->移动数据不如移动计算。

```scala

  /**
   * Optionally overridden by subclasses to specify placement preferences.
   */
  protected def getPreferredLocations(split: Partition): Seq[String] = Nil
```

#### 1.3 执行原理

计算的角度来看，数据处理需要计算资源，包含内存和cpu以及计算模型，真正执行的时候需要将计算资源和计算模型进行协调。Spark框架在执行的时候，先申请资源，然后将应用程序的数据处理逻辑分解成计算任务，然后发送给分配资源的计算节点上，然后按照计算模型进行数据计算，最终得到计算结果。

1）启动yarn集群环境

![image-20210922221137046](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210922221137046.png)

2）Spark申请资源创建调度节点以及计算节点

![image-20210922221131467](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210922221131467.png)

3）Spark框架根据需求将计算逻辑根据分区划分为不同的任务

![image-20210922221123075](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210922221123075.png)

4）调用节点根据节点状态发送task到指定的计算节点计算

![image-20210922221112814](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20210922221112814.png)

RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给
Executor 节点执行计算

### 2 基础编程

#### 2.1 创建RDD

##### 2.1.1 基于集合

```scala
/**
 * 基于内存构建RDD
 * 1)parallelize
 * 2)makeRDD:实际上调用的就是parallelize方法.
 *
 * @author : code1997
 * @date : 2021/9/22 22:13
 */
object RDD_Memory {

  def main(args: Array[String]): Unit = {
    //local[*]根据计算机的硬件条件.
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val sc = new SparkContext(sparkConf)
    val seq = Seq[Int](1, 2, 3, 4)
    val rdd1: RDD[Int] = sc.parallelize(seq)
    val rdd2: RDD[Int] = sc.makeRDD(seq)
    rdd1.collect().foreach(println)
    rdd2.collect().foreach(println)
    sc.stop()
  }
}
```

##### 2.1.2 基于文件

```scala
/**
 * 基于文件创建RDD
 * textFile:一次读取文件中的一行数据.
 * wholeTextFiles：一次读取一个文件，返回值是(文件路径,整个文件内容)
 *
 * @author : code1997
 * @date : 2021/9/22 22:25
 */
object RDD_File {

  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val sc = new SparkContext(sparkConf)
    //textFile不仅仅可以指向具体的文件，而且可仅指向path，也可以使用通配符以及分布式存储系统路径，比如hdfs
    val rdd1: RDD[String] = sc.textFile("data/spark-core/wordcount/1.txt")
    val rdd2: RDD[String] = sc.textFile("E:\\projects\\ideacode\\atguigu\\spark-code1997\\data\\spark-core\\wordcount\\1.txt")
    rdd1.collect().foreach(println)
    rdd2.collect().foreach(println)
    val rdd3: RDD[(String,String)] = sc.wholeTextFiles("data/spark-core/wordcount")
    rdd3.collect().foreach(println)
    sc.stop()
  }

}
```

##### 2.1.3 分区和并行度

1）并行度

默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建 RDD 时指定。如果不传递，就会使用默认值，也称为默认并行度。

```scala
    //local[*]根据计算机的硬件条件.
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val sc = new SparkContext(sparkConf)
    val seq = Seq[Int](1, 2, 3, 4)

    val rdd: RDD[Int] = sc.makeRDD(seq,2);
    rdd.saveAsTextFile("data/spark-core/output/RDD_Memory")
    sc.stop()
```

local模式默认并行度：从`sparkConfig`中获取参数`spark.default.parallelism`，如果获取不到就使用本地最大的运行核数

```scala
  override def defaultParallelism(): Int =
    scheduler.conf.getInt("spark.default.parallelism", totalCores)
```

2）分区

1、读取内存集合数据，数据分区的规则：如果分区很多，但是数据不多，如何处理？？

```scala
// TODO: Right now, each split sends along its full data, even if later down the RDD chain it gets
// cached. It might be worthwhile to write the data to a file in the DFS and read it in the split
// instead.
// UPDATE: A parallel collection can be checkpointed to HDFS, which achieves this goal.

override def getPartitions: Array[Partition] = {
  val slices = ParallelCollectionRDD.slice(data, numSlices).toArray
  slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray
}
```

核心逻辑：`positions`方法，对每个分区求出[start,end).

```scala
private object ParallelCollectionRDD {
  /**
   * Slice a collection into numSlices sub-collections. One extra thing we do here is to treat Range
   * collections specially, encoding the slices as other Ranges to minimize memory cost. This makes
   * it efficient to run Spark over RDDs representing large sets of numbers. And if the collection
   * is an inclusive Range, we use inclusive range for the last slice.
   */
  def slice[T: ClassTag](seq: Seq[T], numSlices: Int): Seq[Seq[T]] = {
    if (numSlices < 1) {
      throw new IllegalArgumentException("Positive number of partitions required")
    }
    // Sequences need to be sliced at the same set of index positions for operations
    // like RDD.zip() to behave as expected
    def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] = {
      (0 until numSlices).iterator.map { i =>
        val start = ((i * length) / numSlices).toInt
        val end = (((i + 1) * length) / numSlices).toInt
        (start, end)
      }
    }
    seq match {
      case r: Range =>
        positions(r.length, numSlices).zipWithIndex.map { case ((start, end), index) =>
          // If the range is inclusive, use inclusive range for the last slice
          if (r.isInclusive && index == numSlices - 1) {
            new Range.Inclusive(r.start + start * r.step, r.end, r.step)
          }
          else {
            new Range(r.start + start * r.step, r.start + end * r.step, r.step)
          }
        }.toSeq.asInstanceOf[Seq[Seq[T]]]
      case nr: NumericRange[_] =>
        // For ranges of Long, Double, BigInteger, etc
        val slices = new ArrayBuffer[Seq[T]](numSlices)
        var r = nr
        for ((start, end) <- positions(nr.length, numSlices)) {
          val sliceSize = end - start
          slices += r.take(sliceSize).asInstanceOf[Seq[T]]
          r = r.drop(sliceSize)
        }
        slices
      case _ =>
        val array = seq.toArray // To prevent O(n^2) operations for List etc
        positions(array.length, numSlices).map { case (start, end) =>
            array.slice(start, end).toSeq
        }.toSeq
    }
  }
}
```

2、读取文件分区